# Word2Vec (CBOW) from Scratch with Wikipedia Data

A from-scratch implementation of the Continuous Bag of Words (CBOW) variant of Word2Vec, trained on Wikipedia text data.  
The project demonstrates low-level neural embedding learning and visualizes learned word vectors using t-SNE.

---

## ğŸ“ Project Structure

All logic â€” from data scraping and preprocessing to model training and visualization â€” is implemented in a single Jupyter Notebook for clarity and step-by-step demonstration.

Python modules (e.g., `model.py`, `train.py`, etc) have been initialized and will be modularized in upcoming updates.

> ğŸ”§ Until then, refer to the notebook for the full working implementation.

---

## Features

- âœ… CBOW model implemented from scratch using PyTorch
- ğŸŒ Raw Wikipedia text fetched via `wikipedia-api`
- ğŸ§­ Visualized embeddings using t-SNE 
- ğŸ—‚ï¸ Modular Python scripts planned
- ğŸ–¥ï¸ Planned: simple Gradio demo to explore word similarities

